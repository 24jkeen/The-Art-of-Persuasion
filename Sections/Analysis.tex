\section{Analysis}

\subsection{Fixed Points}

It is possible to calculate the fixed points of the discrete time mappings that govern the dynamics of the systems and compute their stability. By definition, these points will occur when $\mathbf{P}^{i+1}_L = \mathbf{P}^{i}_L$. In the case of the Open Model, these are trivial to compute. Consider \cref{eq:simple_update_rule}. There is an obvious fixed point at $\alpha = 1$. In this scenario, no matter what argument is presented, the listener will never be able to take that new information into account and will never update its beliefs, giving us a fixed point. To find others, we take $\alpha < 1$ to give a fixed point at $\mathbf{P}^i_L = \mathbf{A}^i_S$. This tells us that our second fixed point will occur when the speaker and the listener have identical probability distributions. To verify the stability of these, we compute the derivative of \cref{eq:simple_update_rule}. This yields

\begin{equation}
    \mathbf{P}^i_L = \alpha \cdot \mathbf{I} .
\end{equation}

Computing the eigenvalues of this equation shows that they are all equivalent to $alpha$. This implies that for our fixed points, $alpha = 1$ is stable and $\mathbf{P}^i_L = \mathbf{A}^i_S$ is stable since we assume that $alpha < 1$, so all the eigenvalues lie within the unit circle. In the case of $\alpha = 1$, the eigenvalues equal $l$, meaning that the system is \emph{not} asymptotically stable. These two facts a supported by simulation and show that, when $\alpha = 1$ nothing happens, and when $\alpha < 1 $ the agents arrive at the same probability distribution that is a function of their initialisation. 

Taking the same approach with the Bottom Up and Top Down models, we can compute their fixed points in a similar way. Using the amended update rule in \cref{eq:BU_update_rule}, we get

\begin{align*}
    \mathbf{P}_L = \alpha \cdot \mathbf{P}_L + (1 - \alpha) \cdot \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}.
\end{align*}

As above, we find one fixed point at $\alpha = 1$. The others can only exist when

\begin{align*}
    \mathbf{P}_L = \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}.
\end{align*}

This tells us a number of things. Firstly, one can intuit from $\mathbf{A} \odot \mathbf{P}L$ that the assertion $\mathbf{A}$ must contain every state of the world for which the listeners $p_i \neq 0$. From that, it is possible to say that $\mathbf{A} \cdot \mathbf{P}_L = 1$ since $\mathbf{A}$ can be represented as a column vector of $1$'s and $0$'s. 

Under these conditions, we compute the stability of the fixed points as follows. 

\begin{align}
    f(\mathbf{P}_L) &= \alpha \cdot \mathbf{P}_L + (1 - \alpha) \cdot \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}\\
    \frac{d f(\mathbf{P}_L)}{d \mathbf{P}_L} &= \alpha \cdot \mathbf{I} + (1 - \alpha) \cdot \frac{d \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}}{d \mathbf{P}_L}\label{eq:BU_stability}
\end{align}

Focusing on the final derivative in the expression, 

\begin{align*}
    \frac{d \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}}{d \mathbf{P}_L} &= \mathbf{A} \odot \mathbf{P}_L \cdot \frac{d \frac{1}{\mathbf{A} \cdot \mathbf{P}_L}}{d \mathbf{P}_L} + \frac{1}{\mathbf{A} \cdot \mathbf{P}_L} \cdot \frac{d \mathbf{A} \odot \mathbf{P}_L}{d \mathbf{P}_L}  \\
    &= \mathbf{A} \odot \mathbf{P}_L \cdot - \frac{\mathbf{A}}{(\mathbf{A} \cdot \mathbf{P}_L)^2} + \frac{1}{\mathbf{A} \cdot \mathbf{P}_L} \cdot \mathbf{A} \\
    &= \frac{\mathbf{A}}{\mathbf{A} \cdot \mathbf{P}_L} \cdot \left(  \mathbf{A} - \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}  \right)
\end{align*}



Substituting this into \cref{eq:BU_stability},

\begin{align*}
    \frac{d f(\mathbf{P}_L)}{d \mathbf{P}_L} &= \alpha \cdot \mathbf{1} + (1 - \alpha) \cdot  \frac{\mathbf{A}}{\mathbf{A} \cdot \mathbf{P}_L} \cdot \left(  \mathbf{A} - \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L}  \right) \\
\end{align*}

As in the first case, the fixed point $\alpha = 1$ has eigenvalues at $\alpha$, again making it stable, though not asymptotically. The second fixed point requires a little simplification. Evaluating this equation at the fixed point gives,

\begin{align*}
    \frac{d f(\mathbf{P}_L)}{d \mathbf{P}_L} &= \alpha \cdot \mathbf{1} + (1 - \alpha) \cdot \frac{\mathbf{A}}{\mathbf{A} \cdot \mathbf{P}_L} \cdot \left(  \mathbf{A} - \mathbf{P}_L  \right)\\
    &= \alpha \cdot \mathbf{1} + (1 - \alpha) \cdot \left( \frac{\mathbf{A} \cdot \mathbf{A}}{\mathbf{A} \cdot \mathbf{P}_L} - \mathbf{1} \right) \\
    &= (2\alpha -1) \cdot \mathbf{1}  + (1 - \alpha) \cdot \frac{\mathbf{A} \cdot \mathbf{A}}{\mathbf{A} \cdot \mathbf{P}_L} \\
    &= (2\alpha -1) \cdot \mathbf{1}  + (1 - \alpha) \cdot \mathbf{A} \cdot \mathbf{A} \\
    &= (2\alpha -1) \cdot \mathbf{1}  + (1 - \alpha) \cdot |\mathbf{A}| \\
\end{align*}



\subsection{Consensus}

To gain an insight into the dynamics of these systems, let us consider their entropy, as defined by \cref{eq:shannon_entropy}. This serves as a measure of the uncertainty in the system such that high values imply the absence of strong beliefs by the agents, and values close to zero implying a level of certainty, though, importantly, not consensus. To illustrate this distinction, consider \cref{fig:eg_bary}. Clearly, there are some agents who happen to be initialised with some strong beliefs in some states of the world. These agents will have low entropy values, whereas those closer to the centre of the plot will have much higher values. 

\begin{figure}[H]
 \centering
  \begin{minipage}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/OpenModel/open_model_BC_n_3_p_100_gullibility_0,3_runs_10.png}
 \end{minipage}
 \caption{ A Barycenter plot showing $100$ uniformly distributed agents in a world with 3 states.  }\label{fig:eg_bary}
\end{figure}


\begin{figure}[H]
 \centering
  \begin{minipage}[ht]{0.49\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/Entropy/ALL_n_3_p_100_gamma_0,5_alpha_0,5.png}
 \end{minipage}
 \hfill
 \begin{minipage}[ht]{0.49\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/ALL_n_3_p_100_gamma_0,5_alpha_0,5.png}
 \end{minipage}
 \caption{Two plots showing how entropy and J-divergence vary over time for the Open, Bottom Up and Top Down models/ These results were obtained using a value of $\alpha = 0.3$}\label{fig:entropy_J_all}
\end{figure}


\Cref{fig:entropy_J_all} shows plots of entropy and J-divergence changing each iteration for each of the three models proposed. From the first plot it can be seen that the Open Model behaves very differently to the other two. In fact, the Open Model increases the entropy within a system, suggesting that the agents, on average, move away from the corners in \cref{fig:eg_bary}, decreasing their beliefs in some possible states of the world. The agents in this model essentially draw together, averaging their beliefs with every iteration that tends towards a uniform distribution. This is a function of the initial distribution though, not an inherent characteristic of the model. This increase in entropy suggests that this model is not a useful means of communication as it does not achieve reveal any strong sentiment in the cohort of agents, reducing their certainty in any possible state of the world to approximately uniform. 

This figure also shows that the Bottom Up and Top Down models behave very similarly. They both decrease the average entropy of the system to close to $0$ over time. These plots were created using $\gamma = 0.5$ to best compare the two models though it is not possible to compare them directly in this way. This figure merely serves to show that all models demonstrate some convergent behaviour over time, and two drive the agents toward high probability masses for at least $1$ state of the world. It is unclear from this plot whether the agents have achieved consensus. 

The second plot serves to demonstrate convergent behaviour. The J-divergence is a symmetric version of KL-divergence, a distance metric well suited to probabilistic distances. This shows that the Open Model very rapidly reduces the distance between the agents, until it reaches $0$ after which there is no change. This fact, combined with the entropy plot show that agents in the simple model cluster together away from the edges of the plot in \cref{fig:eg_bary}. Conversely, the Bottom Up and Top Down models both converge to a single certain belief in one state of the world. 



\subsection{Convergence}

In order to compare the behaviour of the two non-trivial models, consider their rate of convergence. Since they are not directly comparable for the same values of $\gamma$, the rate of change of convergence time against $\gamma$ must be considered. \Cref{fig:convergence_none} shows four such plots. 

\begin{figure}[H]
 \centering
  \begin{subfigure}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/Convergence_ALL_n_3_p_100_gamma_100_runs_20.png}
    \caption{Convergence}
 \end{subfigure}
 \hfill
 \begin{subfigure}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/J_Div_ALL_n_3_p_100_gamma_100_runs_20.png}
    \caption{J-Divergence}
 \end{subfigure}
 \hfill
 \begin{subfigure}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/Entropy_ALL_n_3_p_100_gamma_100_runs_20.png}
    \caption{Entropy}
 \end{subfigure}
 \caption{Three plots showing time to convergence against a range of values for $\gamma$ for the Bottom Up and Top Down model. (\textit{$n=3, \alpha = 0.5, N = 100$ averaged over 20 runs})}\label{fig:convergence_none}
\end{figure}

It is apparent that, for both models, high values of gamma relate to very rapid convergence. This can be attributed to the fact that at values of $\gamma \approx 1$, it is impossible for either model to create a meaningful assertion; they are limited to asserting $\emptyset$ in the Bottom Up model and $\mathbf{W}$ for the Top Down which are both produce no reaction from the listening agent. This registers as very rapid convergence since the entropy does not change over time. 
In the the Bottom Up plots, one can observe that we get rapid convergence for values of $\gamma \geq \frac{1}{2}$. At this point, it becomes impossible for the speaker to assert anything other than a singleton, meaning that its arguments are guaranteed to be precise. This alone will increase the rate of convergence but another factor to consider is that as gamma increases, the only agents that can put forward any non-trivial argument are those who already hold extreme beliefs which high probability in one particular state. This accelerates the movement of the general population towards the region in which extremist speakers can speak until more and more of the population holds similarly strong beliefs and so more of them become able to assert something, perpetuating the effect. 

In the Bottom Up plots it is also possible to observe a sharp increase in convergence time shortly after $ \gamma = \frac{1}{n}$. As $\gamma$ increases above this point, it begins to create a region of probability space in which it is impossible for an agent to assert anything, making them purely passive, voiceless. This will increase the convergence time as, if such an agent is picked, it is incapable of speaking, meanwhile, the agents that can speak are likely to be asserting slightly general statements so convergence is slow. 

The top down approach exhibits some different behaviour with slow convergence times for low values of gamma that appear to decrease for the most part. The slow convergence for low $\gamma$ is attributable oscillations in the dynamics of the system. At these low values, it becomes possible for the agents to assert states for which they have very little probability. This delays convergence by convincing listening agents to update their probability distributions in a way that increases the distance between the speaker and listener. 

The entropy and divergence plots reveal the state of the agents at time of convergence. High entropy implies that some agents do not have strong belief in one state, whereas the average pairwise J-Divergence suggests that they have failed to converge on a single state of the world. It can be seen that, for low values of gamma, we get a variety of different beliefs in the population, and that at least some of them are not certain in one state of the world alone. This is due to the lack of ability in the speaker to create non-zero arguments in the Bottom Up model, but as gamma increases there is a clear decrease in both entropy and J-divergence as both models find more flexibility in their expressions. As $\gamma$ increases, the Bottom Up model becomes less able to express its beliefs, and the extreme speakers are the only ones able to speak, decreasing the convergence times but, as the probability of the entropy changing very little over a number of iterations increases, so does the entropy and J-divergence. 

The Top Down model is shown to be more robust to changes in $\gamma$, with the majority of the agents holding strong beliefs at convergence, although they do not always agree on the strong belief as evidenced by the relatively high value of J-divergence. 

\subsection{Adaptions}

One key flaw in these models is that the agents are never able to ignore the speaker; they must take the assertion into account and react accordingly, even if it does not lead to a change of the listeners beliefs. This can be remedied by applying a threshold below which the listener agent will not attempt to update its beliefs. 

This is implemented by reversing the method by which the speaker creates its assertion. For example, in the Bottom Up model, the speaker includes every state of the world for which it has sufficient probability. In order to be more discerning, the listener will only update if every state in the speakers assertion is above $\gamma$ according to the listener, otherwise it will simply ignore the speaker. 

Similarly, the Top Down model requires that, from the listeners perspective, the speakers assertion has a probability above $\gamma$. The Open Model does not present any behaviour that encourages certainty in the population so it will no longer be considered



\begin{figure}[H]
 \centering
  \begin{subfigure}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/FIE_alpha_0,3_p_100_n_3_runs_20.png}
    \caption{Convergence}
 \end{subfigure}
 \hfill
 \begin{subfigure}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/J_div_FIE_ALL_n_3_p_100_gamm_0,5_alpha_0,5.png}
    \caption{J-Divergence}
 \end{subfigure}
 \hfill
 \begin{subfigure}[ht]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Images/Figures/All/FIE_ALL_n_3_p_100_gamm_0,5_alpha_0,5.png}
    \caption{Entropy}
 \end{subfigure}

 \caption{Three plots showing time to convergence against a range of values for $\gamma$ for the Bottom Up and Top Down model, with discerning listeners. (\textit{$\alpha = 0.5, N = 100$ averaged over 20 runs})}\label{fig:convergence_FIE}
\end{figure}

The plots shown in \cref{fig:convergence_FIE} are little different from \cref{fig:convergence_none}, suggesting that simply passively ignoring a differing speaker does not dramatically affect the convergent behaviour. However, instead of this passive approach of ignoring the speaker if the express an opinion that the listener cannot agree with to some extent, let us consider a more spiteful approach. If the listener is faced with an assertion that it deems to be too opposed to its own beliefs, let the listener update its beliefs using $\mathbf{A}^c$ instead. This is as if the listener reacts spitefully to any point of view it does not agree with. This behaviour could lead to more rapid convergence times as the listeners will interpret every statement in a way that fits with their original beliefs. However, it is also possible this attitude fails to produce any convergent behavior as, once both the speaker and the listener hold strong, seperate beliefs, the speaker will assert a singleton containing only one state, and the listener will be unable to accept that argument, and instead update on an assertion containing every other state of the world making convergence unlikely. 