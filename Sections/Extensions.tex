\section{Extensions}

\subsection{Optimised}
There exists a wide range of possible extensions to the previous section. Here we consider only a few. Firstly, let  the restriction that an agent must maintain a level of integrity and create assertions that are related to its own beliefs be lifted. This allows the agent to tailor its argument to the listener such that the listener will update its set of beliefs to align with the speakers as closely as possible. This can be formulated as a simple Integer Programming problem as follows. 

The speaker constructs an argument by minimizing the J-Divergence between the speaker and listener after the listener updates. This is given by

\begin{equation}
    min \hspace{1.5em} \mathbf{P}_S \cdot \log \left( \frac{\frac{1}{2} (\mathbf{P}_S  + \mathbf{P}_L^{t+1}) }{\mathbf{P}_S} \right) +  \mathbf{P}_L^{t+1} \cdot \log \left( \frac{\frac{1}{2} (\mathbf{P}_S  + \mathbf{P}_L^{t+1}) }{\mathbf{P}_L^{t+1}} \right) 
\end{equation} 

Clearly, the only variable that the speaker can influence in order to minimise this objective function is $\mathbf{A}$, hence, we can rewrite the above as

\begin{equation}
    min \hspace{1.5em}\mathbf{P}_S \cdot \log \left( \frac{\mathbf{Q}}{\mathbf{P}_S} \right) +  \left( \alpha \cdot \mathbf{P}^{i}_L + (1 - \alpha) \cdot  \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L} \cdot \log \left( \frac{\mathbf{Q}}{ \left( \alpha \cdot \mathbf{P}^{i}_L + (1 - \alpha) \cdot  \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L} \right)  } \right) \right)
\end{equation} 

\begin{equation}
    \mathbf{Q} = \frac{1}{ 2} \left(\mathbf{P}_S  +  \alpha \cdot \mathbf{P}^{i}_L + (1 - \alpha) \cdot  \frac{\mathbf{A} \odot \mathbf{P}_L}{\mathbf{A} \cdot \mathbf{P}_L} \right)
\end{equation}

The vector of $1$'s and $0$'s $\mathbf{A}$ must be constructed by the speaker such that the cost function is minimised. This can be passed to an optimisation package and the result becomes the speaker's argument. 




\subsection{Ageing Population}

One aspect of all the above models is the structure of the system remains constant over time, with the only variations coming from $\mathbf{P} \textnormal{ and } \mathbf{A}$. This means that the agents always treat new information the same way, in some cases ignoring it entirely and others incorporating it into their beliefs. Now consider effects of varying $\alpha$ as using in \cref{eq:BU_update_rule}. This parameter affects the importance of new information to the listener. 

In order to limit the impact of new information on an agent, let $\alpha = f(b)$ where $b$ is the number of arguments the agent has heard so far. In order to gradually decrease the impact of new information on a better informed agent, lef $f(b) = 1 - \frac{1}{2b}$. When the listener receives its first argument, it will update using \cref{eq:BU_update_rule} and $\alpha = 0.5$. This number will tend to $1$ such that the listener will be unaffected by new information and will remain firm in its beliefs. 


\subsection{Evidence}

In work many studies on the area of multiagent communication models, there exists a ``true state'' of the world that the agents must confer in order to agree upon. Here we will explore the effects of including such a feature. Firstly, we arbitrarily assign one state of the world to be ``true''. Then at each iteration the simulation conducts $N$ Bernoulli trials. This establishes for each agent whether or not they receive a piece of evidence. Evidence takes the same form as $\mathbf{A}$ and is handled as a standard argument. Once the agents that will receive evidence have been selected, they each query an oracle. The oracle returns a singleton assertion which reflects the true state with some probability $p$. 

\todo{define singletons}