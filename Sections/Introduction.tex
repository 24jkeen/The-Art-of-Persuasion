\newpage

\section{Introduction}

Persuasion is part of daily life for many; whenever an individual comes across an advert, a company is attempting to attract new customers; or even this very document, in which the author is attempting to persuade the reader that the information set out henceforth is reliable, hopefully backed up by rigorous analysis. This sort of persuasion is often studied in Social Sciences, however, as increased attention is directed towards Artificial Intelligence, the lines between Social and Computer Science begins to blur. In this document, we aim to model the interactions between agents to assess different strategies for persuasion. 

Unfortunately, models such as these require assumptions that make them tractable but also incorrect. The first assumption is that an object with a point of view can become part of a so-called ``agent-based'' model in which a population can be simulated by assigning a number of agents a small number of simple rules that allows more complex dynamics to evolve. The second assumption is referred to as the Closed World Assumption (CWA). This assumes that there is a finite number of states that the world can exist in and they are all known to each agent. Under these two assumptions, it becomes possible to model a population of agents with opinions and beliefs about the possibility of each world arising. Hence, the social interactions between two agents can be better understood and demonstrate the ability of a cohort to learn and exchange opinion~\cite{Wooldridge1995IntelligentPractice}. 

This is useful for modelling the dynamics of populations, as well understanding the potential methods by which an intelligent system might make decisions based on uncertain events. This could help mitigate the opacity of the current decision making process of AI's and improve the explainability of intelligent systems. Furthermore, improving the ability for an agent to communicate the most salient information could be essential in a search-and-rescue environment, for instance, if an agent in a search-and-rescue team spotted two clues, one relevant and one likely irrelevant, it should communicate only the relevant clue such that its team can work as quickly and efficiently as possible. 

There exist some high profile examples of argumentative intelligent systems, including IBM's Project Debater and Facebook's negociating agents

\textbf{Introduction / lit review should be done nearer the end as it is a function of the work done beforehand to some extent}

